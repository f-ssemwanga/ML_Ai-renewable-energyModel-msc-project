# -*- coding: utf-8 -*-
"""RenewableEnergy_UK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EbmdqNqlGdlfYArahjZ1ln4EzjimxUC5
"""

'''
importing required libraries to import and perform machine learning on the dataset
'''
import pandas as pd
import numpy as np

#splitting the dataset into training and testing sets 30% testing and 70 percent training
from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV

#used to predict to predict continuous variables as there was no categorical data
from sklearn.ensemble import RandomForestRegressor
#Used to improve the model performance  and accuracy of the model
from sklearn.metrics import mean_squared_error
#library used for feature selection because it gives best results for continous variables
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression,f_classif
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report  as cls_report,confusion_matrix
from sklearn.metrics import accuracy_score, classification_report  as cls_report,confusion_matrix
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
from matplotlib import pyplot
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# Load the renewable energy dataset

# this data has dependent and independent variables
renewable_energy_df = pd.read_csv('/content/drive/MyDrive/DOC-20230529-WA0021_/uk_renewable_energy.csv')
renewable_energy_df

# Load the CO2 emissions dataset
co2_emissions_df = pd.read_csv('/content/drive/MyDrive/DOC-20230529-WA0021_/GCB2022v27_MtCO2_flat.csv')
co2_emissions_df

# Merge the datasets on the common column 'Year' as it is similar in both files
merged_df = pd.merge(renewable_energy_df, co2_emissions_df, on='Year')
merged_df

# Handle missing values in CO2 emissions columns - these were checked manually
# simple imputter is use to add the missing values by using the mean stragey for different columns
imputer = SimpleImputer(strategy='mean')
merged_df[['Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other', 'Per Capita']] = imputer.fit_transform(
    merged_df[['Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other', 'Per Capita']])
merged_df

# Select relevant columns for modeling
# select the features - independent variables needed to predict the outcome
features = ['Energy from renewable & waste sources', 'Total energy consumption of primary fuels and equivalents',
            'Fraction from renewable sources and waste', 'Hydroelectric power', 'Wind, wave, tidal',
            'Solar photovoltaic', 'Geothermal aquifers', 'Landfill gas', 'Sewage gas', 'Biogas from autogen',
            'Municipal solid waste (MSW)', 'Poultry litter', 'Straw', 'Wood', 'Charcoal', 'Liquid bio-fuels',
            'Bioethanol', 'Biodiesel', 'Biomass', 'Cross-boundary Adjustment','Coal','Oil','Cement','Gas','Flaring','Other','Per Capita','Year']
target = 'Total'  # Total CO2 emissions - this is our outcome variable

# Split the data into training and testing sets using a testing ratio of 30%
X = merged_df[features]
y = merged_df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X,y
#X_train,X_test,y_train,y_test

# model 1: Linear Regression used for continuous variables
lr_classifier = LinearRegression()
lr_classifier.fit(X_train, y_train)
lr_y_val_pred = lr_classifier.predict(X_test) #in X_test we have independent testing variables
df_preds = pd.DataFrame({'Actual': y_test.squeeze(), 'Predicted': lr_y_val_pred.squeeze()})
print(df_preds)

#checking the accuracy of the model using mean square error
mae = mean_absolute_error(y_test, lr_y_val_pred)
mse = mean_squared_error(y_test, lr_y_val_pred)
rmse = np.sqrt(mse)
print(f'Mean absolute error: {mae:.2f}')
print(f'Mean squared error: {mse:.2f}')
print(f'Root mean squared error: {rmse:.2f}')

# Visualize the predicted CO2 emissions vs. actual CO2 emissions
plt.figure(figsize=(10, 6))
plt.scatter(lr_y_val_pred, y_test, color='b', alpha=0.5, label='Data Points')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='y', linestyle='--', label='Ideal Line')
plt.xlabel('Actual CO2 Emissions')
plt.ylabel('Predicted CO2 Emissions')
plt.title('Predicted vs. Actual CO2 Emissions (Full Data)')
plt.legend()
plt.show()

# Step 4: Cross-validation and evaluation of the above model
# used to test and train the model on different iterations and this is performed on the full dataset
scores_model = cross_val_score(lr_classifier, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
rmse_model = np.sqrt(-scores_model.mean())
fig, ax = plt.subplots()

# Bar plot for Model without Feature Selection
ax.bar(range(len(scores_model)), -scores_model, alpha=0.5)
ax.axhline(y=-rmse, color='red', linestyle='--', linewidth=2)
ax.set_xlabel("Fold")
ax.set_ylabel("Negative Mean Squared Error")
ax.set_title("Model without Feature Selection")
plt.show()

# Step 3: Model with feature selection
selector = SelectKBest(score_func=f_regression, k=10)  # Adjust k as needed i.e. we used 10 as it is a standard value for this dataset
X_selected = selector.fit_transform(X_train, y_train)
selected_features = X_train.columns[selector.get_support()]
model_fs = LinearRegression()
model_fs.fit(X_selected, y_train)

#Check mean square error for the feature selection task
X_selected_fs = selector.transform(X_train)
scores_model_fs = cross_val_score(model_fs, X_selected_fs, y_train, cv=5, scoring='neg_mean_squared_error')
rmse_model_fs = np.sqrt(-scores_model_fs.mean())
std_rmse_model_fs = scores_model_fs.std()

print("RMSE (Model with feature selection): {:.2f}".format(rmse_model_fs))
print("Standard Deviation of RMSE (Model with feature selection): {:.2f}".format(std_rmse_model_fs))

# Bar plot for Model with Feature Selection
fig, ax = plt.subplots()
ax.bar(range(len(scores_model_fs)), -scores_model_fs, alpha=0.5)
ax.axhline(y=-rmse_model_fs, color='red', linestyle='--', linewidth=2)
ax.set_xlabel("Fold")
ax.set_ylabel("Negative Mean Squared Error")
ax.set_title("Model with Feature Selection")
plt.show()

# Second model: Random Forest Regressor on Full Data
model_full = RandomForestRegressor()
model_full.fit(X_train, y_train)
y_pred_full = model_full.predict(X_test)
mse_full = mean_squared_error(y_test, y_pred_full)
r2_full = r2_score(y_test, y_pred_full)

print("Model 1 (Full data):")
print("   Mean Squared Error:", mse_full)
print("   R-squared Score:", r2_full)

# Perform k-fold cross-validation for the full model
full_model_scores = cross_val_score(model_full, X, y, cv=5, scoring='r2')
print("Full Model Cross-Validation Scores:")
print(full_model_scores)
print("Mean R^2 Score (Full Model):", np.mean(full_model_scores))

#Third Model:  Create an SVR model
svm_regressor = SVR()

# Fit the model on the training data
svm_regressor.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = svm_regressor.predict(X_test)

# Calculate the root mean squared error (RMSE)
rmse = np.sqrt(mean_squared_error(y_test, y_val_pred))
print("Support Vector Machine Regression RMSE: {:.2f}".format(rmse))

# Plot the predicted values vs. the actual values
plt.scatter(y_test, y_val_pred, alpha=0.5, label='Predicted vs. Actual')
plt.plot(y_test, y_test, color='red', linestyle='--', linewidth=2, label='Ideal Line')
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Support Vector Machine Regression - Actual vs. Predicted")
plt.legend()
plt.show()

# Step 2: Feature Selection
selector = SelectKBest(score_func=f_regression, k=10)  # Adjust k as needed
X_selected = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()]
# Display the names of the selected features
for feature in selected_features:
    print(feature)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

##Bar Plot of CO2 Emissions by Fuel Type
fuel_types = ['Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other']
emissions = [merged_df['Coal'].sum(), merged_df['Oil'].sum(), merged_df['Gas'].sum(),
             merged_df['Cement'].sum(), merged_df['Flaring'].sum(), merged_df['Other'].sum()]

plt.figure(figsize=(10, 6))
plt.bar(fuel_types, emissions)
plt.xlabel('Fuel Type')
plt.ylabel('CO2 Emissions')
plt.title('CO2 Emissions by Fuel Type')
plt.grid(False)
plt.show()

#Stacked Area Plot of Renewable Energy Sources Over Time on full dataset
renewable_sources = ['Hydroelectric power', 'Wind, wave, tidal', 'Solar photovoltaic',
                     'Geothermal aquifers', 'Landfill gas', 'Sewage gas', 'Biogas from autogen',
                     'Municipal solid waste (MSW)', 'Poultry litter', 'Straw', 'Wood', 'Charcoal',
                     'Liquid bio-fuels', 'Bioethanol', 'Biodiesel', 'Biomass', 'Cross-boundary Adjustment']

plt.figure(figsize=(10, 6))
plt.stackplot(merged_df['Year'], [merged_df[source] for source in renewable_sources], labels=renewable_sources)
plt.xlabel('Year')
plt.ylabel('Energy Production')
plt.title('Renewable Energy Sources Over Time')
plt.legend(loc='upper left')
plt.grid(False)
plt.show()

#Line Plot of Total CO2 Emissions Over Time
plt.figure(figsize=(10, 6))
plt.bar(merged_df['Year'], merged_df['Total'])
plt.xlabel('Year')
plt.ylabel('Total CO2 Emissions')
plt.title('Total CO2 Emissions Over Time')
plt.grid(False)
plt.show()

#Bar Plot of Fraction from Renewable Sources and Waste
plt.figure(figsize=(10, 6))
plt.bar(merged_df['Year'], merged_df['Fraction from renewable sources and waste'])
plt.xlabel('Year')
plt.ylabel('Fraction from renewable sources and waste')
plt.title('Fraction from Renewable Sources and Waste Over Time')
plt.grid(False)
plt.show()

#Heatmap of Correlation Matrix
numeric_columns = merged_df.select_dtypes(include=np.number)
corr_matrix = numeric_columns.corr()

plt.figure(figsize=(20, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()